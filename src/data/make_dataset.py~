
# -*- coding: utf-8 -*-
import os
from pymongo import MongoClient
from pymongo.errors import BulkWriteError
from bson  import Binary
from PIL import Image
import pandas as pd
from io import BytesIO
from time import time
import re
import tifffile as tiff
import pickle

if __name__=='__main__':
    project_home='/home/pk/kaggle/Amazon/planet_amazon/'
    client=MongoClient()
    db=client.data
    train=db.train
    test=db.test
    
    
    descr=pd.read_csv(project_home+'data/train.csv')
    
    i=0
    j=0
    bulk=train.initialize_ordered_bulk_op()
    
    if not(os.path.exists(project_home+'data/train_list.txt')):
        open_file=open(project_home+'data/train_list.txt','w+')
        file_list=[f for f in os.listdir(project_home+'data/train_tif')]
    else:
        open_file=open(project_home+'data/train_list.txt','r')
        file_list=open_file.readlines()
    file_list=[x.strip('\n') for x in file_list]
    to_keep=file_list
    for file_name in file_list:
        if not(re.search("[0-9]+",file_name)):
            continue
        time_now=time()
        #im=Image.open(project_home+'data/train_tif/'+file_name)
        #im_byte=BytesIO()
        #im.save(im_byte,"tiff")
        try:
            im=tiff.imread(project_home+'data/train_tif/'+file_name)
           #im=tiff.imread(project_home+'data/train_tif/train_36250.tif')
        except:
            print (file_name +" skipped ")
            j=j+1
            continue
        im_byte=Binary(pickle.dumps(im,protocol=2))
        tags=descr.ix[descr["image_name"]==file_name[:-4],"tags"]
        if not(tags.empty):
            tags=tags.values[0].split()
        else:
            tags=""
        document={"name":file_name ,
                  "type":file_name[-3:] ,
                  "tags":tags,
                  "image":im_byte
        }
        
        bulk.insert(document)
        i+=1              
    
        try:
            bulk.execute()                       
            to_keep.remove(file_name)            
        except BulkWriteError as bwe:
            #print(bwe.details)
            print(file_name+" had bulk write errors")
        bulk=train.initialize_ordered_bulk_op()
    try:
        open_file.truncate()
        open_file.close()
        os.remove(project_home+'data/train_list.txt')        
    except:
        print("file doesn't exist")        
    open_file=open(project_home+'data/train_list.txt','w') 
    for f in to_keep:
        open_file.write(f)
        open_file.write('\n')
        
        '''
        if i%3000==0:
            bulk.execute()
            print (str(i)+" files processed : "+ file_name+" in "+ str((time()-time_now))+" sec ")
            bulk=db.train.initialize_ordered_bulk_op()
            
    bulk.execute()
    print(str(j)+" files skipped")
    i=0
    j=0
    print ("processing test files")
    bulk=db.test.initialize_ordered_bulk_op()
    for file_name in os.listdir(project_home+'data/test_tif'):    
        if not(re.search("[0-9]+",file_name)):
            continue
        time_now=time()
        #im=Image.open(project_home+'data/test_tif/'+file_name)
        #im_byte=BytesIO()
        #im.save(im_byte,"tiff")
        try:
            im=tiff.imread(project_home+'data/train_tif/'+file_name)
        except:
            print (file_name +" skipped ")
            j=j+1
            continue 
        im_byte=Binary(pickle.dumps(im,protocol=2))
        document={"name":file_name ,
                  "type":file_name[-3:] ,             
                  "image":im_byte
        }
        bulk.insert(document)
        i+=1
        if i%3000==0:
            bulk.execute()
            print (str(i)+" files processed : "+ file_name+" in "+ str((time()-time_now))+" sec ")
            bulk=db.test.initialize_ordered_bulk_op()

bulk.execute()
'''


'''
import os
import click
import logging



@click.command()
@click.argument('input_filepath', type=click.Path(exists=True))
@click.argument('output_filepath', type=click.Path())
def main(input_filepath, output_filepath):
    """ Runs data processing scripts to turn raw data from (../raw) into
        cleaned data ready to be analyzed (saved in ../processed).
    """
    logger = logging.getLogger(__name__)
    logger.info('making final data set from raw data')


if __name__ == '__main__':
    log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    logging.basicConfig(level=logging.INFO, format=log_fmt)

    # not used in this stub but often useful for finding various files
    project_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)

    # find .env automagically by walking up directories until it's found, then
    # load up the .env entries as environment variables
    load_dotenv(find_dotenv())

    main()
'''
